%\documentclass{article}
\documentclass[5pt]{article}

\usepackage{fancyhdr}
\usepackage{extramarks}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{tikz}
\usepackage[plain]{algorithm}
\usepackage{algpseudocode}
\usepackage{enumitem}
\usepackage{mathtools}
\usepackage{hyperref}
\usepackage[all]{hypcap} % fixes the wrong figure jumping issue

%% matlab stuff
\usepackage{listings}
\usepackage{color} %red, green, blue, yellow, cyan, magenta, black, white
\definecolor{mygreen}{RGB}{28,172,0} % color values Red, Green, Blue
\definecolor{mylilas}{RGB}{170,55,241}



\usetikzlibrary{automata,positioning}

%
% Basic Document Settings
%

\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=6.5in
\textheight=9.0in
\headsep=0.25in

\linespread{1.1}

\pagestyle{fancy}
\lhead{\hmwkAuthorName}
\chead{\hmwkClass\ (\hmwkClassInstructor): \hmwkTitle}
%\rhead{\firstxmark}
%\lfoot{\lastxmark}
%\cfoot{\thepage}

\renewcommand\headrulewidth{0.4pt}
\renewcommand\footrulewidth{0.4pt}


\setlength\parindent{0pt}

%
% Create Problem Sections
%

\newcommand{\enterProblemHeader}[1]{
    \nobreak\extramarks{}{Problem \arabic{#1} continued on next page\ldots}\nobreak{}
    \nobreak\extramarks{Problem \arabic{#1} (continued)}{Problem \arabic{#1} continued on next page\ldots}\nobreak{}
}

\newcommand{\exitProblemHeader}[1]{
    \nobreak\extramarks{Problem \arabic{#1} (continued)}{Problem \arabic{#1} continued on next page\ldots}\nobreak{}
    \stepcounter{#1}
    \nobreak\extramarks{Problem \arabic{#1}}{}\nobreak{}
}

\setcounter{secnumdepth}{0}
\newcounter{partCounter}
\newcounter{homeworkProblemCounter}
\setcounter{homeworkProblemCounter}{1}
\nobreak\extramarks{Problem \arabic{homeworkProblemCounter}}{}\nobreak{}

%
% Homework Problem Environment
%
% This environment takes an optional argument. When given, it will adjust the
% problem counter. This is useful for when the problems given for your
% assignment aren't sequential. See the last 3 problems of this template for an
% example.
%
%\newenvironment{homeworkProblem}[1][-1]{
%    \ifnum#1>0
%        \setcounter{homeworkProblemCounter}{#1}
%    \fi
    %\section{Problem \arabic{homeworkProblemCounter}}
%    \setcounter{partCounter}{1}
    %\enterProblemHeader{homeworkProblemCounter}
%}{
    %\exitProblemHeader{homeworkProblemCounter}
%}

%
% Homework Details
%   - Title
%   - Due date
%   - Class
%   - Section/Time
%   - Instructor
%   - Author
%

\newcommand{\hmwkTitle}{Homework\ \#1}
\newcommand{\hmwkDueDate}{April 28, 2016}
\newcommand{\hmwkClass}{Stats 202C}
\newcommand{\hmwkClassInstructor}{Professor S.C. Zhu}
\newcommand{\hmwkAuthorName}{Eric Chuu}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\lnorm}{\left\Vert\left}
\newcommand{\rnorm}{\right\Vert\right}
%
% Title Page
%

\title{
    %\vspace{2in}
    \textmd{\textbf{\hmwkClass:\ \hmwkTitle}}\\
    \vspace{0.1in}\large{\textit{\hmwkClassInstructor\ \hmwkClassTime} \\ Assignment: 1-4}
    \author{\textbf{\hmwkAuthorName} \\ UID: 604406828}
}

%\author{\textbf{\hmwkAuthorName} \\ UID: 604406828}

\renewcommand{\part}[1]{\textbf{\large Part \Alph{partCounter}}\stepcounter{partCounter}\\}

%
% Various Helper Commands
%

% Useful for algorithms
\newcommand{\alg}[1]{\textsc{\bfseries \footnotesize #1}}

% For derivatives
\newcommand{\deriv}[1]{\frac{\mathrm{d}}{\mathrm{d}x} (#1)}

% For partial derivatives
\newcommand{\pderiv}[2]{\frac{\partial}{\partial #1} (#2)}

% Integral dx
\newcommand{\dx}{\mathrm{d}x}

% Alias for the Solution section header
\newcommand{\solution}{\textbf{\large Solution}}

% Probability commands: Expectation, Variance, Covariance, Bias
\newcommand{\E}{\mathrm{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Bias}{\mathrm{Bias}}
\newcommand{\isum}{\sum_{i=1}^n}
\newcommand{\jsum}{\sum_{j=1}^n}
\newcommand{\infint}{\int_{-\infty}^{\infty}}
\newcommand{\bin}{\mathrm{Bin}}
\newcommand{\geom}{\mathrm{Geom}}
\newcommand{\poi}{\mathrm{Poi}}
\newcommand{\expo}{\mathrm{Exp}}
\newcommand{\mgfx}{M_X(t)}
\newcommand{\mgfy}{M_Y(t)}
\newcommand{\pr}{\mathbf{Pr}}
\newcommand{\xvec}{\mathbf{x}}
\newcommand{\yvec}{\mathbf{y}}
\newcommand{\xbar}{\bar{X}}
\newcommand{\ybar}{\bar{Y}}
\newcommand{\samplechi}{\frac{(n-1)s^2}{\sigma^2}}
\newcommand{\ypi}{y: \pi(y) > 0}
\newcommand{\xpi}{x: \pi(x) > 0}


\newcommand{\vertii}[1]{{\left\vert\kern-0.25ex\left\vert #1 
    \right\vert\kern-0.25ex\right\vert}}

\newcommand{\verti}[1]{{\left\vert #1 
    \right\vert}}   

\newcommand\floor[1]{\lfloor#1\rfloor}
\newcommand\ceil[1]{\lceil#1\rceil}

%\[
%  d(x,y) = \left\{\def\arraystretch{1.2}%
%  \begin{array}{@{}c@{\quad}l@{}}
%    {0  & \text{if $x = y$}\\
%    {1 & \text{for $x < y$}\\
%    {2 & \text{for $x > y$}\\
%  \end{array}\right.
%\]




\begin{document}

\lstset{language=Matlab,%
    %basicstyle=\color{red},
    breaklines=true,%
    morekeywords={matlab2tikz},
    keywordstyle=\color{blue},%
    morekeywords=[2]{1}, keywordstyle=[2]{\color{black}},
    identifierstyle=\color{black},%
    stringstyle=\color{mylilas},
    commentstyle=\color{mygreen},%
    showstringspaces=false,%without this there will be a symbol in the places where there is a space
    numbers=left,%
    numberstyle={\tiny \color{black}},% size of the numbers
    numbersep=9pt, % this defines how far the numbers are from the text
    emph=[1]{for,end,break},emphstyle=[1]\color{red}, %some words to emphasise
    %emph=[2]{word1,word2}, emphstyle=[2]{style},    
}




\maketitle


\section{Problem 1} \\

\textbf{(1)}  The period of a state $i$ is given by
\begin{align}
	& d_i = \mathrm{gcd} \{ n \geq 1 : K_{ii}^{n} > 0 \}
\end{align}
We also know that a state $i$ is aperiodic if $d_i = 1$. \\

For $K_0$, we draw the transition graph diagram and see that all states can be reached in finite time with nonzero probability, so it is irreducible. Since the Markov chain is irreducible, then all states have the same period. Thus, it suffices to find the period of any state. If we begin in state 1, we can return to state 1 in two steps by going to state 2 and then back to state 1. We can also return to state 1 in three steps by going to state 2 then state 3, then returning back to state 1 directly. Then, $d_1 = 1$, and state 1 is aperiodic, and we conclude that $K_0$ is aperiodic. \\

For $K_1$, we can see from the transition graph diagram, $d_i = 1$ for $i = 1, \ldots, 5$, so $K_1$ is aperiodic. We can see that $K_1$ is reducible by considering states 5 and 4. If we are in states 4 or 5, the probability of going into states 1, 2, or 3 is 0. \\

For $K_2$, we draw the transition graph diagram and see that all states can be reached in finite time with nonzero probability, so it is irreducible. For each state, we can see that $d_i = 2$ for $i = 1, \ldots, 5$, so $K_2$ is periodic. \\

\textbf{(2)} The eigenvalues and left eigen-vectors for each matrix are given below. \\

$K_0$ left-eigenvectors (stored as column vectors):
\begin{align*}
	\begin{bmatrix}
		-0.167	&-0.07-0.118i   &-0.07+0.118i &	  0.402	  &0.503 \\
		-0.517	& 0.148+0.379i	&0.148-0.379i &	  0.491	 &-0.159 \\
		-0.632	& 0.3-0.241i	    &0.3+0.241i	  & 0.139	 &-0.638 \\
		-0.492	&-0.735	        &-0.735	      & -0.667 	 &-0.221 \\
		-0.251	& 0.357-0.019i	&0.357+0.019i &	 -0.365	 & 0.516
	\end{bmatrix}, \quad \Lambda_1 = 
	\begin{bmatrix}
		1	\\
		-0.544+0.182i	\\
		-0.544-0.182i	\\
		0.279	\\
		-0.19
	\end{bmatrix}
\end{align*}

$K_1$ left-eigenvectors (stored as column vectors):
\begin{align*}
	\begin{bmatrix}
		0		&	-0.284	&	0		&	0.78		&	-0.697 \\
		0		&	-0.42	&	0		&	-0.528	&	0\\
		0		&	-0.386	&	0		&	-0.201	&	0.697\\
		-0.707	&	0.541	&	-0.707	&	-0.213	&	0.116 \\
		-0.707	&	0.549	&	0.707	&	0.162	&	-0.116
	\end{bmatrix}, \quad \Lambda_2 = 
	\begin{bmatrix}
		1	\\
		0.939	\\
		-0.4	\\
		-0.139	\\
		0.2	
	\end{bmatrix}
\end{align*}


$K_2$ left-eigenvectors (stored as column vectors):
\begin{align*}
	\begin{bmatrix}
		-0.149	&	0.149	&	0.772	&	-0.242	&	-0.242 \\
		-0.45	&	0.45		&	-0.617	&	0.645	&	0.645 \\
		-0.46	&	0.46		&	-0.154	&	-0.403	&	-0.403 \\
		0.563	&	0.563	&	0		&	0.426i	&	0.426i \\
		0.497	&	0.497	&	0		&	0.426i	&	0.426i
	\end{bmatrix}, \quad \Lambda_2 = 
	\begin{bmatrix}
		-1	\\
		1	\\
		0	\\
		0.529i	\\
		0.529i	
	\end{bmatrix}
\end{align*}


\textbf{(3)} For $K_0$, there is one invariant probability:
\begin{align*}
	\begin{bmatrix}
		0.08094	&	0.2512		&	0.3071	&	0.2389	&	0.1219
	\end{bmatrix}
\end{align*}
with eigenvalue 1.


For $K_1$, there is one invariant probability:
\begin{align*}
	\begin{bmatrix}
		0	&	0  &	  0	&	0.5000	&	0.5000
	\end{bmatrix}
\end{align*}
with eigenvalue 1.

For $K_2$, there is one invariant probability:
\begin{align*}
	\begin{bmatrix}
		0.0703  &	0.2125	& 0.2172 	& -0.2656	-0.2344
	\end{bmatrix}
\end{align*}
with eigenvalue 1.


\textbf{(4)} The eigen-values for $K_0, K_0^{50}, K_{0}^{200}$ are plotted below. \\

\begin{figure}[H]
\begin{center}
\includegraphics[width = 1 \columnwidth]{eigen_values}
\caption{Left: Eigenvalues for $K_0$. Center: Eigenvalues for $K_0^{50}$. Right: Eigenvalues for $K_0^{200}$}
\end{center}
\end{figure}


\textbf{(5)} We compute the matrix $K_0^{200}$, and we see that it becomes the ``ideal'' matrix, where each row is $\pi$.

\begin{align*}
K_0^{200} = 
	\begin{bmatrix}
		0.809 & 0.2512 & 0.3071 & 0.2389 & 0.1219 \\		
		0.809 & 0.2512 & 0.3071 & 0.2389 & 0.1219 \\	
		0.809 & 0.2512 & 0.3071 & 0.2389 & 0.1219 \\	
		0.809 & 0.2512 & 0.3071 & 0.2389 & 0.1219 \\	
		0.809 & 0.2512 & 0.3071 & 0.2389 & 0.1219
	\end{bmatrix}
\end{align*}


\section{Problem 2}

\textbf{(1)} The plot of $d_{\mathrm{TV}}(n), d_{\mathrm{KL}}(n)$ for the first 200 steps is shown below.

\textbf{(2)} We calculate the contraction coefficient $K_0$ using
\begin{align}
	& C(K_0) = \max_{x, y} \vertii{K_0(x, \cdot) - K_0(y, \cdot)}_{\mathrm{TV}}
\end{align}

For two initial probbailities $\nu_1, \nu_2$,
\begin{align}
	& \vertii{\nu_1 \cdot K_0 - \nu_2 \cdot K_0}_{\mathrm{TV}} \leq C(K_0) \vertii{\nu_1 - \nu_2}_{\mathrm{TV}}
\end{align}

As $\vertii{\nu_1 - \nu_2}_{\mathrm{TV}} \leq 1$, if $C(K) < 1$, then the convergence rate can be upper bounded by
\begin{align}
	& A(n) = \vertii{\nu_1 \cdot K_0^n - \nu_{2} \cdot K_0^n}_{\mathrm{TV}} 
	\leq C^n(K_0) \vertii{\nu_1 - \nu_2}_{\mathrm{TV}} \leq C^n(K_0), \quad \forall \nu_1, \nu_2
\end{align}

The plot for the bound $C^n(K_0)$ over $n = 1, \ldots, 100$ is shown below.

\textbf{(3)} The Diaconis-Hanlon bound is given by
\begin{align}
	& B(n) = \vertii{\pi - \nu K_0^n}_{\mathrm{TV}} \leq \sqrt{\frac{1 -\pi(x_0)}{4 \pi (x_0)}} \lambda_{\mathrm{slem}}^n
\end{align}
where $x_0 = 1$ is the initial state and $\pi(x_0)$ is the target probability at $x_0 = 1$ and $\lambda_{\mathrm{slem}}^n$ is the second largest eigen-value modulus. The plot of the real convergence $d_{\mathrm{TV}}$ in comparison with $A(n), B(n)$ is given below. \\


We see that both converge to 0 very quickly, with KL-Divergence converging to 0 slightly faster. Bound $A(n)$, calculated using the contraction coefficient is shown in the turquoise curve below, while Bound $B(n)$, the Diaconis-Hanlon bound, is shown in the purple curve below. We can see that $B(n)$ provides a tighter bound on the convergence than does $A(n)$. \\

\begin{figure}[H]
\begin{center}
\includegraphics[width = 0.9\columnwidth]{q2_plot}
\caption{TV-norm and KL-Divergence for $n = 200$ steps. Bound A and Bound B refer to $A(n), B(n)$, respectively, both as calculated above.}
\end{center}
\end{figure}



\pagebreak


\section{Problem 3}
In a finite state $\Omega$, suppose at step $t$, a Markov chain MC has state $X$ following probability $\nu$. By applying the Markov kernel $P$ once, its state in $t+1$ is $Y$ which follows probability $\mu = \nu \cdot P$. $P(x,y)$ is the transition (conditional) probability from state $x$ to $y$. Suppose we denote by $Q(x,y)$ the reverse transition probability of $P$ at time $t$, 
\begin{align}
	& Q(y,x) = \frac{P(x,y) \nu(x)}{\mu(y)}
\end{align}

Show that the Kullback-Leibler divergence decreases monotonically,
\begin{align}
	& KL \left( \pi || \nu \right) - KL \left( \pi || \mu \right) = \E\left[
	KL \left( P \left( y, x \right) || Q\left( y, x \right) \right] \geq 0
\end{align}


\textbf{Solution} \\
By definition of KL-divergence, we have
\begin{align*}
	& KL \left( \pi || \nu \right) = \sum_{\ypi} \pi(y) \cdot \log \frac{\pi(y)}{\mu(y)} 
	= \sum_{\ypi} \pi(y) \log \frac{\pi}{\sum_{x} P(x,y) \nu(x)}
\end{align*} 

Note that if $\mu(y) > 0$ when $\pi(y) > 0$, then by definition the KL divergence is infinite. Using this fact, we know that if
$\sum_{x} P(x,y) \nu(x) = 0$ when $\pi(x) > 0$, this implies that $KL(\pi || \nu)$ is infinite and the inequality in (7) holds. If the sum is nonzero, then we have

\begin{align*}
	\sum_{\ypi} \pi(y) \log \frac{\pi}{\sum_{x} P(x,y) \nu(x)}
	&\leq - \sum_{\ypi} \pi(y) \log \left[\sum_{\xpi} \frac{\nu(x)}{\pi(y)} \right]\\
	&\leq - \sum_{\ypi} \pi(y) \log \left[ \sum_{\xpi} Q(y,x) \cdot \frac{\nu(x)}{\pi(x)} \right] \\
	&\leq - \sum_{\ypi} \pi(y) \sum_{\xpi} Q(y,x) \cdot \log \frac{\nu(x)}{\pi(x)} \qquad \textnormal{(Jensen's Inequality)} \\
	&= \sum_{x} \pi(x) \log \frac{\pi(x)}{\nu(x)} \\
	&= KL \left( \pi || \nu \right)
\end{align*}
where the second line follows from applying the equality in (6). Thus we conclude that
\begin{align*}
	KL \left( \pi || \mu \right) & \leq KL \left( \pi || \nu \right) \\
	KL \left( \pi || \nu \right) &- KL \left( \pi || \mu \right) \geq 0
\end{align*}
and the inequality in (7) holds. \hfill $\qed$

\pagebreak


\section{Problem 4}
A Markov chain returning time $\tau_{\mathrm{ret}(i)}$ is the minimum steps that a Markov chain needs to return to state $i$ after leaving state $i$. Suppose we consider a random walk in the countable set of non-negative numbers $\Omega = \{ 1, 2, \ldots,  \}$. At a step, the Markov chain state $x_t = n$, it has probability $\alpha$ to go up ($x_{t+1} = n + 1$) and probability $1 - \alpha$ to return to $x_{t+1} = 0$. Calculate the probability for returning to state 0 in finite step
\begin{align}
	& \pr \left( \tau_{\mathrm{ret}}(0) < \infty \right) = \sum_{\tau(0) = 1}^{\infty} \pr \left( \tau(0) \right)
\end{align}

Calculate the expected return time
\begin{align}
	& \E \left[ \tau_{\mathrm{ret}}(0) \right]
\end{align}


\textbf{Solution} \\
We consider the transition matrix,
\begin{align*}
K = 
	\begin{bmatrix}
		1 - \alpha  & \alpha & 0 & 0      & \cdots \\		
		1 - \alpha  & 0 & \alpha & 0      & \cdots \\	
		1 - \alpha  & 0 & 0      & \alpha & \cdots \\	
		1 - \alpha  & 0 & 0      & 0      & \cdots \\	
		1 - \alpha  & 0 & 0      & 0      & \cdots \\
		\vdots  & \vdots & \vdots  & \vdots  & \vdots 
	\end{bmatrix}
\end{align*}

Each step follows a bernoulli distribution with probability of failure $\alpha$ and probability of success $1 - \alpha$. Then the probability of returning to state 0 in finite step is equivalent to calculating the probability of success in finite step. Using the summation in (8) above, we can calculate 
\begin{align*}
	\pr \left( \tau_{\mathrm{ret}}(0) < \infty \right) &= (1 - \alpha) + \alpha(1 - \alpha) + \alpha^2(1 - \alpha) + \cdots + \alpha^n(1 - \alpha) + \cdots \\
	&= (1 - \alpha) \left[ 1 + \alpha + \alpha^2 + \alpha^3 + \cdots \right] \\
	&= (1 - \alpha) \cdot \frac{1}{1-\alpha}
\end{align*} 
if $\alpha < 1$ (note we disregard absolute value since $\alpha$ is a probability). Thus, the probability of returning to state 0 in finite time is 1 if $\alpha < 1$, but 0 for $\alpha = 1$. The expected return time is then just the expectation of a geometric random variable, i.e., the expected number of times it takes for the first success, so

\[ \E \left[ \tau_{\mathrm{ret}}(0) \right]
\begin{cases}   
	\frac{1}{1 - \alpha}, \quad \textnormal{if } 0 \leq \alpha < 1 \\
	\infty, \qquad \textnormal{if } \alpha = 1
\end{cases}
\]

We see that the expectation is finite only for $\alpha < 1$, and infinite for $\alpha = 1$. \hfill $\qed$

\end{document}





